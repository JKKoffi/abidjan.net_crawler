{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crawl_by_File.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOrnO6O1BGzNRciFhYRppUz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"awaxflRRZIrt"},"source":["# Crawl Abidjan.net annonce by ID\r\n","\r\n","We'll use an ID to crawl and preprocess all of these annonce."]},{"cell_type":"markdown","metadata":{"id":"Lt4God_bZjxc"},"source":["## Imports  libraries"]},{"cell_type":"code","metadata":{"id":"KXqmBfeRZi39"},"source":["import time\r\n","import os\r\n","import numpy as np  \r\n","from datetime import datetime\r\n","import pandas as pd\r\n","\r\n","import io\r\n","import requests\r\n","from string import punctuation\r\n","import locale\r\n","from datetime import datetime\r\n","import re\r\n","from bs4 import BeautifulSoup, SoupStrainer\r\n","import requests\r\n","import sys"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5GbwFobZqys"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"q5SIm4_EZB5S"},"source":["def create_dir(dir_path):\r\n","  if not os.path.exists(dir_path):\r\n","    os.makedirs(dir_path)\r\n","  print(\"Done\")\r\n","\r\n","#mesure elapsed time\r\n","from contextlib import contextmanager\r\n","from timeit import default_timer\r\n","@contextmanager\r\n","def elapsed_timer():\r\n","    start = default_timer()\r\n","    elapser = lambda: default_timer() - start\r\n","    yield lambda: elapser()\r\n","    end = default_timer()\r\n","    elapser = lambda: end-start\r\n","\r\n","# def save_history(df):\r\n","      \r\n","#       if not os.path.exists(path_to_history):\r\n","#         df.to_csv(path_to_history, header=True, encoding='utf-16le', index =False,mode='a')\r\n","#       else:\r\n","#         df1 = pd.read_csv(path_to_history, encoding='utf-16le')\r\n","#         df_full = pd.concat([df1, df],axis=0)\r\n","#         df_full = df_full.reset_index()\r\n","#         df_full = df_full.drop_duplicates()\r\n","#         df.to_csv(path_to_history, header=True, encoding='utf-16le', index =False,mode='a')\r\n","\r\n","\r\n","def search_links(df_merged):\r\n","\r\n","  links = df_merged['link'].dropna().apply(lambda x:float(str(x).split('/')[-1].split('.')[0])).tolist()\r\n","\r\n","  start = int(df_merged['link'].dropna().apply(lambda x:float(str(x).split('/')[-1].split('.')[0])).min())\r\n","\r\n","  pages = []\r\n","  j=start\r\n","  stop = False\r\n","  i = 0\r\n","  while stop ==False:\r\n","\r\n","    if j not in links:\r\n","      url = f'https://business.abidjan.net/AL/a/{j}.asp'\r\n","      response = requests.get(url)\r\n","      if response.ok:\r\n","        pages.append(url)\r\n","        i = 0\r\n","        print(f'\\rCompleted: {url}')\r\n","      else:\r\n","        i +=1\r\n","        if i>50: #stop if more than 50 pages are failled\r\n","          stop=True\r\n","      j +=1\r\n","  df = pd.DataFrame({'pages':pages})\r\n","  return df\r\n","\r\n","def elapsed_timer():\r\n","    start = default_timer()\r\n","    elapser = lambda: default_timer() - start\r\n","    yield lambda: elapser()\r\n","    end = default_timer()\r\n","    elapser = lambda: end-start\r\n","\r\n","\r\n","\r\n","\"\"\" Crawle page data\"\"\"\r\n","\r\n","import re\r\n","from bs4 import BeautifulSoup, SoupStrainer\r\n","import requests\r\n","\r\n","def crawl_data(df_links):\r\n","\r\n","  session = requests.Session()\r\n","\r\n","  data = []\r\n","  mesure_complexity = {} # jsut to measure the execution time\r\n","  pages_fail = []\r\n","\r\n","  N=df_links.shape[0]\r\n","  i = 0\r\n","\r\n","\r\n","  # with elapsed_timer() as elapsed:    \r\n","  # if i>4:\r\n","  #     time.sleep(20)\r\n","  for link in df_links.iloc[:,0].tolist():\r\n","    data1 = []\r\n","    download_url = link\r\n","    data1.append(download_url)\r\n","    try:\r\n","      response = session.get(download_url)\r\n","      try:\r\n","        soup = BeautifulSoup(response.text, 'html.parser')\r\n","        # elem = soup.find(\"div\", {\"id\": \"module\"})\r\n","        # data1 = [\r\n","        #           download_url,\r\n","        #           elem.next_sibling.next_sibling.next_sibling.next_sibling.text,\r\n","        #           elem.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.text\r\n","        #         ]\r\n","        elem = soup.findAll(\"div\", {\"id\": \"module\"})\r\n","        data1 = [\r\n","                  download_url,\r\n","                  elem[0].text,elem[1].text,elem[2].text\r\n","                ]\r\n","        print(len(data1[2]))    \r\n","        data.append(data1)\r\n","        print(f'{download_url} done!')\r\n","      except:\r\n","        try:\r\n","          soup = BeautifulSoup(response.text, 'html.parser')\r\n","          # elem = soup.find(\"div\", {\"id\": \"shadow\"})\r\n","          elem = soup.find(\"div\", {\"id\": \"page_content\"})\r\n","          data1 = [download_url,elem.text]\r\n","          print(len(data1[1]))\r\n","          data.append(data1)\r\n","        except:\r\n","          print(f'failed on {download_url}')\r\n","          pages_fail.append(download_url)\r\n","    except:\r\n","        pages_fail.append(download_url)\r\n","    # time.sleep(5)\r\n","    i +=1\r\n","    prog = ((i+1)/N) * 100\r\n","    print('\\rCompleted: {:.2f}%'.format(prog),end=' ')\r\n","\r\n","\r\n","\r\n","  df_2 = pd.DataFrame({'link':[x[0] for x in data if len(x)==2], \"content\":[x[1] for x in data if len(x)==2]})\r\n","  df_3 = pd.DataFrame({'link':[x[0] for x in data if len(x)==4], \"header\":[x[2] for x in data if len(x)==4],\"content\":[x[3] for x in data if len(x)==4]})  \r\n","  df_fail = pd.DataFrame({'pages':pages_fail}) \r\n","  df_fail.to_csv(os.path.join(dir_result,'pages_failed.csv'), header=True, encoding='utf-16le', index =False , sep=';',mode=\"a\")\r\n","\r\n","  df_3['page-content'] = df_3['header']+''+df_3['content']\r\n","  df_2['page-content'] = df_2['content']\r\n","  df_merged = pd.DataFrame()\r\n","  columns=['link', 'page-content']\r\n","  df_merged = pd.concat([df_2.loc[:,columns], df_3.loc[:,columns]], axis=0)\r\n","  df_merged = df_merged.reset_index()\r\n","  df_merged = df_merged.drop(['index'], axis = 1)\r\n","\r\n","  return df_merged\r\n","\r\n","\r\n","import unicodedata\r\n","\r\n","\r\n","def remove_accents(input_str):\r\n","    nfkd_form = unicodedata.normalize('NFKD', input_str)\r\n","    only_ascii = nfkd_form.encode('ASCII', 'ignore')\r\n","    return only_ascii.decode('utf-8')\r\n","# src:https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\r\n","\r\n","from string import punctuation\r\n","def remove_punctuations(string):\r\n","\tfor pct in punctuation:\r\n","\t\tstring = string.replace(pct,' ')\r\n","\t\tstring = re.sub('\\s\\s+',' ',string)\r\n","\treturn string\r\n","\r\n","\r\n","def decode_encode(string):\r\n","  bytestring = string.encode('ascii', 'ignore').decode('ascii')#.encode('utf-8')\r\n","  return bytestring#.decode('utf-8')\r\n","\r\n","#find email\r\n","def find_email(string):\r\n","  try:\r\n","    email_found = re.findall(r'[\\w]*@[\\w]*.[\\w]*', string)\r\n","    if len(email_found)==1:\r\n","      return email_found[0]\r\n","    else:\r\n","      chr = ''\r\n","      for x in email_found:\r\n","        chr += '/'+ x\r\n","      return chr\r\n","  except:\r\n","    pass\r\n","\r\n","#find capital\r\n","def find_capital(string):\r\n","\r\n","  string = remove_punctuations(string)\r\n","  string = string.replace('FRANCS CFA','FCFA')\r\n","  # string = string.replace('F CFA', 'FCFA')\r\n","  string0 = string.replace(' ','')\r\n","  string0 = string0.replace('DE','')\r\n","  try:\r\n","    return float(re.findall(r'[0-9]{7,12}FCFA', string0)[0].replace('FCFA', ''))\r\n","  except:\r\n","    try:\r\n","      start = string.lower().find('capital')\r\n","      string1 = string1[start,start+15]\r\n","      return float(re.findall(r'[0-9]{7,12}', string0)[0].replace('FCFA', ''))\r\n","    except:\r\n","      pass\r\n","\r\n","def find_tel(string):\r\n","  string = remove_accents(string).lower()\r\n","  start = string.find('tel')\r\n","  if start!=-1:\r\n","    string = string[start:]\r\n","    string = remove_punctuations(string)\r\n","    # string = string.replace('225','')\r\n","    stop = string.find('capital') \r\n","    string = string[:stop]\r\n","    chr = ''\r\n","    string = string.replace(' ','')\r\n","    string = re.findall(r'[0-9]{8,8}', string)\r\n","    for x in string:\r\n","      chr += x+'|'\r\n","    return chr\r\n","\r\n","\r\n","  string = string\r\n","  string = remove_punctuations(string)\r\n","  # string = string.replace('225','')\r\n","  stop = string.find('fax') \r\n","  string = string[:stop]\r\n","  chr = ''\r\n","  string = string.replace(' ','')\r\n","  string = re.findall(r'[0-9]{8,8}', string)\r\n","  for x in string:\r\n","    chr += x+'|'\r\n","  return chr\r\n","\r\n","\r\n","def find_fax(string):\r\n","  string = remove_accents(string).lower()\r\n","  start = string.find('fax')\r\n","  if start!=-1:\r\n","    string = string[start:]\r\n","    string = remove_punctuations(string)\r\n","    string = string.replace('225','')\r\n","    string = string[:20]\r\n","    chr = ''\r\n","    string = string.replace(' ','')\r\n","    string = re.findall(r'[0-9]{8,8}', string)\r\n","    for x in string:\r\n","      chr +=' '+x\r\n","    return chr\r\n","\r\n","def find_notaire(string):\r\n","  start = string.lower().find('notaire')\r\n","  string = string[start:start+50]\r\n","  string0 =  re.findall(r'[A-Z]{2,20}',string)\r\n","  if len(string0)!=0:\r\n","    chr = ''\r\n","    for x in string0:\r\n","      chr += ' '+x\r\n","    return chr\r\n","  else:\r\n","    stop = string.find('date')\r\n","    string1 = string[:stop]\r\n","    start = string1.find(':')\r\n","    try:\r\n","      int(string1[start:])\r\n","    except:\r\n","      return string1[start:]\r\n","\r\n","\r\n","def find_cat(string):\r\n","  start = string.lower().find('catégorie')\r\n","  string = string[start:start+100]\r\n","  stop=string.lower().find('avocat')\r\n","  string = string[:stop-1]\r\n","  string0 =  re.findall(r'[A-Z]{2,20}',string)\r\n","  if len(string0)!=0:\r\n","    chr = ''\r\n","    for x in string0:\r\n","      chr += ' '+x\r\n","    return chr\r\n","\r\n","def find_cat(string):\r\n","  start = remove_accents(string).lower().find('catégorie')\r\n","  if start!=-1:\r\n","    \r\n","    string = string[start:start+100]\r\n","    stop=string.lower().find('avocat')\r\n","    string = string[:stop-1]\r\n","    string0 =  re.findall(r'[A-Z]{2,20}',string)\r\n","    if len(string0)!=0:\r\n","      chr = ''\r\n","      for x in string0:\r\n","        chr += ' '+x\r\n","      return chr\r\n","\r\n","  start = remove_accents(string).lower().find('catgorie')\r\n","  string = string[start:start+100]\r\n","  stop=string.lower().find('avocat')\r\n","  string = string[:stop-1]\r\n","  string0 =  re.findall(r'[A-Z]{2,20}',string)\r\n","  if len(string0)!=0:\r\n","    chr = ''\r\n","    for x in string0:\r\n","      chr += ' '+x\r\n","    return chr\r\n","\r\n","\r\n","def find_date(string):\r\n","  string = string.lower()\r\n","  start = string.lower().find('date de')\r\n","  if start!=-1:\r\n","    try:\r\n","      string = string[start:start+50]\r\n","      string = re.findall(r'[a-z]+ [0-9]{1,2} [a-z]+ [0-9]{4,4}',string)[0]\r\n","      return string\r\n","    except:\r\n","      pass\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","def find_rccm(string):\r\n","  string = string.lower()\r\n","\r\n","  start = string.find('rccm')\r\n","  if start!=-1:\r\n","    try:\r\n","      string = string[start:start+100].replace(' ','') \r\n","      string = re.findall(r'[a-z]*[a-z]*[0-9]{1,4}[a-z]{1,1}[0-9]{1,4}', remove_punctuations(string))\r\n","      string0 =  string[0].replace('rccm','') \r\n","      stop = string0.find('ci')\r\n","      return string0[stop:]\r\n","    except:\r\n","      pass\r\n","  try:\r\n","    return re.findall(r'[a-z]*[a-z]*[0-9]{1,4}[a-z]{1,1}[0-9]{1,4}', remove_punctuations(string))\r\n","  except:\r\n","    pass\r\n","\r\n","\r\n","def remove_punctuations2(string):\r\n","  for pct in punctuation:\r\n","    string = string.replace(pct,' ')\r\n","    string = re.sub('\\s\\s+',' ',string)\r\n","  return string\r\n","\r\n","def find_miss_rccm(string):\r\n","  string = string.encode('ascii', 'ignore').decode('ascii')\r\n","  string =  string.lower()\r\n","  string0 = re.findall(r'[a-z]* [a-z]* [a-z]*[0-9]{1,4} [a-z]{1,1} [0-9]{1,4}', remove_punctuations2(string))\r\n","  if len(string0)>0:\r\n","      return string0[0]\r\n","  string1 = re.findall(r'[a-z]*[a-z]*[0-9]{1,4}[a-z]{1,1} [0-9]{1,6}', remove_punctuations2(string))\r\n","  if len(string1)>0:\r\n","        return string1[0]\r\n","\r\n","def find_missing_rccm(string):\r\n","  fields_list = [x for x in re.split('\\n[^\\w]*', string.split('\\n\\n\\n')[-1]) if x not in [' ',''] and len(x)>2]\r\n","  for field in fields_list:\r\n","    field = remove_accents(field.lower())\r\n","    start = (remove_accents(field.lower())).find(remove_accents('rccm'.lower()))\r\n","    if start!=-1:\r\n","      return field[start+4:]\r\n","\r\n","#BP\r\n","def find_bp(string):\r\n","  string = string.lower()\r\n","  start = remove_accents(string).find('siege')\r\n","  if start!=-1:\r\n","    matches = ['[0-9]{1,4} bp [0-9]{1,4} [a-z]* [0-9]{1,2}',r'[0-9]{1,4} bp [0-9]{1,4}',r'[0-9]{1,2} bp [0-9]{1,4} [a-z]* [0-9]{1,4}',r'bp [0-9]{1,4} [a-z]* [0-9]{1,4}',r'bp [0-9]{1,4} [a-z]* [0-9]{1,4}',r'[a-z]* bp [0-9]{1,4}',r'[a-z]* bp [0-9]{1,4}']\r\n","\r\n","    for match in matches:\r\n","      # string = string[start:]#start+100]\r\n","      string0 = re.findall(match, string)\r\n","      if len(string0)!=0:\r\n","        chrt = ''\r\n","        for x in string0:\r\n","          chrt += x + \"/\"\r\n","        return chrt\r\n","  else:\r\n","    matches = [r'bp [a-z]+ [0-9]+',r'\\[a-z]{2,15}',r'[0-9]{1,4} bp [0-9]{1,4} [a-z]* [0-9]{1,2}',r'[0-9]{1,4} bp [0-9]{1,4}',r'[0-9]{1,2} bp [0-9]{1,4} [a-z]* [0-9]{1,4}',r'bp [0-9]{1,4} [a-z]* [0-9]{1,4}',r'bp [0-9]{1,4} [a-z]* [0-9]{1,4}',r'[a-z]* bp [0-9]{1,4}',r'[a-z]* bp [0-9]{1,4}']\r\n","\r\n","    for match in matches:\r\n","      # string = string[start:]#start+100]\r\n","      string0 = re.findall(match, string)\r\n","      if len(string0)!=0:\r\n","        chrt = ''\r\n","        for x in string0:\r\n","          chrt += x + \"|\"\r\n","        return chrt\r\n","\r\n","# string = df_merged.loc[84084,'page-content']\r\n","def find_miss_bp(string):\r\n","  string = string.lower()\r\n","  start = string.find('bp')\r\n","  if start!=-1:\r\n","    string = string[start:]\r\n","    stop = string.find('\\n')\r\n","    if stop!=-1:\r\n","      string = string[:stop]\r\n","      return re.sub('\\s\\s+','',remove_punctuations(string).replace('bp',''))#.find('\\n')\r\n","        \r\n","def find_duree(string):\r\n","  start = (remove_accents(string).lower()).find('duree')\r\n","  if start!=-1:\r\n","      string0 = string[start+8:start+12]\r\n","      string0 = re.findall(r'[0-9]{2,2}',string0)\r\n","      if len(string0)==1:\r\n","        return string0[0]\r\n","      string1 = string[start:start+12]\r\n","      string1 = re.findall(r'[0-9]{2,2}',string1)\r\n","      return string1\r\n","      \r\n","    \r\n","\r\n","      \r\n","def find_name(string):\r\n","  start = remove_accents(string).lower().find('societe')\r\n","  string = string[start+8:start+100]\r\n","  stop=remove_accents(string).lower().find('categorie')\r\n","  string = string[:stop-1]\r\n","  string0 =  re.findall(r'[A-Z]{2,20}',string)\r\n","  if len(string0)!=0:\r\n","    chr = ''\r\n","    for x in string0:\r\n","      chr += ' '+x\r\n","    return chr\r\n","  else:\r\n","    string0 =  re.findall(r'[A-Z]{1,20}',string)\r\n","    chr = ''\r\n","    for x in string0:\r\n","      chr += ' '+x\r\n","    return chr\r\n","\r\n","def find_siege(string):\r\n","  string = remove_accents(string).lower() \r\n","  start = string.find('siege')\r\n","  string = string[start:]\r\n","  stop = string.find('\\n')\r\n","  string = string[:stop]\r\n","  string = string.replace('siege','')\r\n","  string = string.replace('social','')\r\n","  string = string.replace(':','')\r\n","  string = re.sub('\\s\\s+','',string)\r\n","  return string  \r\n","\r\n","\r\n","\r\n","def search_fileds(df_merged):\r\n","\r\n","  df_merged['content'] = df_merged['page-content'].apply(lambda x: ' '.join(x.split('\\n')))\r\n","\r\n","  df_merged['content'] = df_merged.content.apply(lambda string: decode_encode(string) )\r\n","\r\n","  #replace multiple space by one\r\n","  df_merged['content'] = df_merged['content'].apply( lambda string: re.sub(\"\\s\\s+\" , \" \", string) )\r\n","\r\n","  # df_merged['email'] = \r\n","  df_merged['email'] = df_merged['content'].apply( lambda string: find_email( string) )\r\n","\r\n","  # df_merged['email'] = \r\n","  df_merged['capital'] = df_merged['content'].apply( lambda string: find_capital(string) )\r\n","\r\n","  df_merged['tel'] = df_merged['content'].apply(lambda string: find_tel(string))\r\n","\r\n","  df_merged['fax'] = df_merged['content'].apply(lambda string: find_fax(string))\r\n","\r\n","  df_merged['notaire'] = df_merged['content'].apply(lambda string: find_notaire(string))\r\n","\r\n","  df_merged['notaire'] = df_merged['content'].apply(lambda string: find_notaire(string))\r\n","\r\n","  df_merged['categorie'] = df_merged['content'].apply(lambda string: find_cat(string))\r\n","\r\n","  df_merged['nom'] = df_merged['content'].apply(lambda string: find_name(string))\r\n","\r\n","  df_merged['date'] = df_merged['content'].apply(lambda string: find_date(string))\r\n","\r\n","  df_merged['rccm'] = df_merged['content'].apply(lambda string: find_rccm(string))\r\n","\r\n","  df_merged.loc[df_merged['rccm'].isna(),'rccm'] = df_merged.loc[df_merged['rccm'].isna(),'content'].apply(lambda x:find_miss_rccm(x))\r\n","\r\n","  df_merged.loc[df_merged['rccm'].isna(),'rccm'] = df_merged.loc[df_merged['rccm'].isna(),'page-content'].apply(lambda x:find_missing_rccm(x))\r\n","\r\n","  df_merged['bp'] = df_merged['content'].apply(lambda string: find_bp(string))\r\n","\r\n","  df_merged.loc[df_merged['bp'].isna(),'bp'] = df_merged.loc[df_merged['bp'].isna(),'page-content'].apply(lambda x:find_miss_bp(x))\r\n","\r\n","  df_merged['duree'] = df_merged['content'].apply(lambda string: find_duree(string))\r\n","\r\n","  df_merged.loc[[i for i,x in enumerate(df_merged['content']) if x.lower().find('abidjan')!=-1],['ville']]='Abidjan'\r\n","\r\n","  df_merged['siege'] = df_merged['page-content'].apply(lambda x: find_siege(x))\r\n","\r\n","\r\n","  df_merged.loc[[i for i,x in enumerate(df_merged['content']) if x.lower().find('abidjan')!=-1],['ville']]='Abidjan'\r\n","  \r\n","\r\n","  return df_merged\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pbMrqzPZ3vj"},"source":["## Set up  dirs and csts, and local language"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"0lkcNNhtaLda","executionInfo":{"status":"ok","timestamp":1608630031780,"user_tz":0,"elapsed":6997,"user":{"displayName":"Jonathan koffi","photoUrl":"","userId":"05826095740746588787"}},"outputId":"ab66a58a-530e-4e9d-d561-94bd2b9e6343"},"source":["dir_result = 'results'\r\n","path_to_history = os.path.join(dir_result, 'history.csv')\r\n","create_dir(dir_result)\r\n","#set for french date\r\n","locale.setlocale(locale.LC_ALL)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=C;LC_COLLATE=C;LC_MONETARY=C;LC_MESSAGES=C;LC_PAPER=C;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=C;LC_IDENTIFICATION=C'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"HqIKPsOdafd7"},"source":["## Start search lin by File\r\n","\r\n","* Search links and save into a variable called `df_link`\r\n","\r\n","* Start Id will be the min id in entry links.\r\n","\r\n","* need to give the absolute path of your file name"]},{"cell_type":"code","metadata":{"id":"vq5fPzG7Z21O"},"source":["path_to_csv = '/content/annonceDetail1.csv'#path_to_csv = 'absolute path to your csv'\r\n","df_merged = pd.read_csv(path_to_csv, encoding='utf-16le' )\r\n","df_links = search_links(df_merged)\r\n","# df_links.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4JqbUa2dWUS"},"source":["## Start the crawler to grab content for each links found"]},{"cell_type":"code","metadata":{"id":"C22TZtFMc-y_"},"source":["df_merged = crawl_data(df_links)\r\n","df_merged.head()#will return the 5 rows of the df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S0WFjW2iimvi"},"source":["### Search fileds and return df"]},{"cell_type":"code","metadata":{"id":"YsM23CiwegC1"},"source":["\r\n","df_merged = search_fileds(df_merged)\r\n","df_merged.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hg2DGdAwjYRj"},"source":["## Display missing fields in df\r\n"]},{"cell_type":"code","metadata":{"id":"1iaGAUQYhqHw"},"source":["df_merged.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xjgz_9Lijlj2"},"source":["#Update  your dataframe or save"]},{"cell_type":"code","metadata":{"id":"B_O4gAuXjitE"},"source":["#we load the data into df_merged1 and update it\r\n","path_to_df_1 = '/content/annonceDetail1.csv'#path_to_df_1 = 'Enter the path of your existing df'\r\n","df_merged1 = pd.read_csv(path_to_df_1, encoding='utf-16le' )\r\n","df_merged1.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVmZR3tplTF0"},"source":["df_merged_complete = pd.concat([df_merged1, df_merged],axis = 0)\r\n","df_merged_complete = df_merged_complete.reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFe1j-kjoUIK","executionInfo":{"status":"ok","timestamp":1608559739441,"user_tz":0,"elapsed":813,"user":{"displayName":"Jonathan koffi","photoUrl":"","userId":"05826095740746588787"}},"outputId":"b0b7866e-8bbe-4f0d-f8d8-ce357f46d704"},"source":["type(stringlist)==list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"Hhm8oxBapVHv"},"source":["* Transform list into values"]},{"cell_type":"code","metadata":{"id":"Xj_4NqDWnvAV"},"source":["def transform_list(stringlist):\r\n","\r\n","  if type(stringlist)==list:\r\n","    chr = ''\r\n","    for x in stringlist:\r\n","      chr += x +'|'\r\n","    return chr\r\n","  return stringlist\r\n","\r\n","for col in df_merged_complete.columns.tolist():\r\n","  df_merged_complete[col] = df_merged_complete[col].apply(lambda x:transform_list(x))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o57is-i1pfOe"},"source":["_*Pay attention to the path where you'll save the updated file*_"]},{"cell_type":"code","metadata":{"id":"kahxezKgmiZV"},"source":["df_merged_complete = df_merged_complete.drop_duplicates()\r\n","path_to_save_df = '/content/annonceDetail_udate.csv'#path_to_save_df = 'write gere the absolute path of your new df'\r\n","df_merged_complete.to_csv(path_to_save_df, header=True, encoding='utf-16le', index =False )"],"execution_count":null,"outputs":[]}]}